{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from langdetect import detect, DetectorFactory\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = '../data/data_intermediaire_train.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Sea Is Watching has been made from an original Akira Kurosawa script, and it is indeed a lush and warm film. Watching it will be a pleasure !<br /><br />Kei Kumai as director is certainly no equal to the old but everlasting master (particularly the mass scenes in the beginning of the film has some terrible acting), but the overall mood and scenery is very enjoyable. Another thing that is missed here: Kurosawa always managed to let the characters be so much more then what they are actually showing and doing.<br /><br />Probably that was his magic on set while shooting; and just maybe this script was not fully up to par yet.<br /><br />Maybe we just miss the eye of the master.<br /><br />This is one lovely and sweet film, but it is no Kurosawa. To expect that might well be very silly...'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet(input)\n",
    "df.iloc[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "df_vect = vectorizer.fit_transform(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding pour le mot 'incroyable': None\n",
      "Sentiment prédit pour le nouveau document: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Vectorisation des documents\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df['text'])\n",
    "\n",
    "# Réduction de la dimensionnalité avec TruncatedSVD (similaire à LSA)\n",
    "svd = TruncatedSVD(n_components=50)\n",
    "X_reduced = svd.fit_transform(X)\n",
    "\n",
    "# Apprentissage des embeddings de mots\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "word_vectors = svd.components_.T\n",
    "\n",
    "# Apprentissage du modèle de sentiment\n",
    "model = LogisticRegression()\n",
    "model.fit(X_reduced, df['label'])\n",
    "\n",
    "# Fonction pour obtenir le vecteur d'un mot\n",
    "def get_word_vector(word):\n",
    "    if word in vocab:\n",
    "        idx = np.where(vocab == word)[0][0]\n",
    "        return word_vectors[idx]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "# Prédiction de sentiment pour un nouveau document\n",
    "new_document = [\"this movie is incredible\"]\n",
    "new_document_vec = vectorizer.transform(new_document)\n",
    "new_document_reduced = svd.transform(new_document_vec)\n",
    "predicted_sentiment = model.predict(new_document_reduced)\n",
    "print(f\"Sentiment prédit pour le nouveau document: {predicted_sentiment[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les données\n",
    "file_path = '../data/data_intermediaire_train.parquet'\n",
    "df = pd.read_parquet(file_path)\n",
    "\n",
    "# Ajouter des stopwords supplémentaires si nécessaire\n",
    "additional_stopwords = set([\"movie\",\"film\"])\n",
    "stopwords = STOPWORDS.union(additional_stopwords)\n",
    "\n",
    "# Créer un nuage de mots pour chaque catégorie (0: mauvais film, 1: bon film)\n",
    "for label, category in [(0, \"Mauvais film\"), (1, \"Bon film\")]:\n",
    "    # Filtrer les données pour la catégorie actuelle\n",
    "    category_data = df[df['label'] == label]['text'].str.cat(sep=' ')\n",
    "\n",
    "    # Générer le nuage de mots\n",
    "    wordcloud = WordCloud(stopwords=stopwords, background_color='white', width=800, height=400).generate(category_data)\n",
    "\n",
    "    # Afficher le nuage de mots\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Word Cloud for {category}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "\n",
    "DetectorFactory.seed = 0  \n",
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return 'unknown'\n",
    "\n",
    "df['language'] = df['text'].apply(detect_language)\n",
    "\n",
    "language_counts = df['language'].value_counts()\n",
    "print(\"Distribution des langues détectées :\")\n",
    "print(language_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.language=='nl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(text):\n",
    "    if isinstance(text, str):  \n",
    "        words = text.split()  \n",
    "        return len(words)\n",
    "    return 0\n",
    "\n",
    "# Ajouter une colonne pour le nombre de mots\n",
    "df['word_count'] = df['text'].apply(count_words)\n",
    "\n",
    "# Statistiques sur le nombre de mots\n",
    "word_count_distribution = df['word_count'].describe()\n",
    "print(\"\\nStatistiques sur le nombre de mots :\")\n",
    "print(word_count_distribution)\n",
    "\n",
    "# Visualisation de la distribution du nombre de mots\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(df['word_count'], bins=50, alpha=0.7, color='blue')\n",
    "plt.title('Distribution du nombre de mots par revue')\n",
    "plt.xlabel('Nombre de mots')\n",
    "plt.ylabel('Fréquence')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
